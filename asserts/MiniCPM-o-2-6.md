# MiniCPM-o 2.6原理解析
虽然本项目主要探索文本/音频转换模型技术，MiniCPM-o 2.6模型处理支持文本和视觉模态外，也能支持音频输入输出，并且其支持全双工交流和对话打断，功能极其强劲，属于GPT-4o级别的模型，值得深入研究。以下内容是对OpenBMB公布的[技术报告](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9)的提炼，对更多细节感兴趣的可以阅读原文。

 - [介绍](#介绍)
 - [架构](#架构)

## 介绍
&emsp;&emsp;MiniCPM-o 2.6是OpenBMB在MiniCPM-v系列模型基础上开发的多模态大语言模型/MLLM，能接收图片、视频、文本和音频输入，并以端到端的方式输出高质量文本和音频。虽然总参数量只有8B，但MiniCPM-o 2.6在多视觉、语音和多模态流式输出方面能力与GPT-4o-202405相当。具体来说，MiniCPM-o 2.6的显著特点包括：
 - 领先的视觉能力：单图、多图和视频理解能力都很强，并且有良好的上下文学习能力
 - SOTA的语音能力：支持中/英文的双语实时语音对话，具有强劲的音频理解理解能力，还支持情感、语速、风格控制，端到端声音克隆，角色扮演等
 - 强大的多模态流式能力：能接收独立于用户queries的连续视频和音频流输出，支持实时语音交互
 - 强大OCR及其他能力：能处理任意尺寸比的图片，最高支持180万像素；在OCRBench中获得25B参数量以下模型的SOTA效果；基于RLAI-V和VisCPM技术，在MMHal-Bench上表现优于GPT-4o和Claude 3.5 Sonnet，并支持30多种语言的视觉语言功能
 - 卓越的性能：具有强劲的token密度(多少个像素可以编码为一个视觉token)，可将一张180万像素图片处理为640个视觉tokens，比大多数模型少75%，直接提高推速度，首token延迟和所需内存等

&emsp;&emsp;人类可持续从环境中感知视觉、音频和文本流信息，低延迟的产生语音和文本输出，这对MLLM来说是据答的挑战，MiniCPM-o 2.6是如何实现多模态实时交互的呢？
  1. 将原来的多模态信息离线编解码转换为在线的流式编解码：大多数模态编码器和解码器都是离线的，意味着LLM backbones必须等待所有的视频/音频输入全部编码结束后才能进行推理，并且还需要等待所有的tokens预测结束后才能进行后续解码操作。MiniCPM-o 2.6在时间维度上将不同模态的流式输入/输出划分为小的chunks，实现以chunk的方式进行编码/解码来降低延迟。与CosyVoice2中提出的[Chunk-aware Flow Matching](CosyVoice.md#chunk-aware-flow-matching)类似，允许解码器在第一段文本chunk预测结束后就能立即生成音频
  2. 提出一种全模态时分复用机制处理并行多模态流：借鉴通信领域中时分复用技术，按1s一个chunk将每个模态的流进行切分，将同一时刻的内所有模态信息打包，使得LLM backbone可以实时高效处理多模态信息

&emsp;&emsp;大量传统语音对话模型基于ASR-LLM-TTS管道构建，因为ASR导致失去大量的信息，如用户的情感、环境音等。端到端的模型可以直接处理音频流，但面临较低的数据和计算效率问题。MiniCPM-o 2.6如何实现语音对话中高效的端到端语音、情绪、语调控制？
 1. 采用轻量级的语音解码器，通过密集的隐藏表示和文本标记以混合方式与LLM连接。LLM 生成的隐向量确保了语音监督可以以端到端的方式反向传播到整个模型参数，从而实现更高的潜力。文本连接提供了强大的语义控制，并减少了对训练数据的需求。为了学习丰富且细粒度的语音知识，在大规模自然语音数据上进行预训练，然后将模型与用户指令对齐。

## 架构